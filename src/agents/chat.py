"""Front-facing conversational agent."""

import logging

from pydantic_ai import Agent, RunContext

from src.agents.skill import create_skill_judge_agent
from src.skills import evaluate_recent_conversation, get_user_skill_summary
from src.structs import ChatDeps

log = logging.getLogger(__name__)

# TODO: Use @agent.instructions (supports RunContext) to modify system prompt dynamically.


def create_chat_agent():
    """Create front-facing chat agent."""
    agent = Agent(
        # "groq:meta-llama/llama-4-scout-17b-16e-instruct",
        "google-gla:gemini-2.5-flash-lite-preview-06-17",
        deps_type=ChatDeps,
        # TODO: The list of social skills shouldn't be so specific, or otherwise dynamically retrieved from the attainable social skills.
        # TODO: Isolate all prompt info to a file that reads from .md files for easier swapping.
        instructions=f"""\
Your name is Buddy. You are a supportive conversation partner designed to help users practice social skills.

Your primary goals:
1. Engage in natural, supportive conversations
2. Help users practice social skills through organic interaction
3. Observe when users demonstrate social skills and evaluate their progress

When you notice a user demonstrating social skills during conversation, use the judge_conversation tool to evaluate their performance. Look for moments when the user shows:
- Active listening
- Assertiveness
- Empathy
- Conversation initiation
- Conflict resolution
- Emotional regulation
- Social awareness
- Encouragement
- Boundary setting
- Small talk skills

Don't judge every message - only when you observe clear demonstrations of social skills. Be encouraging and constructive in your responses.

Pay attention to any discrepancies between what you know about the user's profile (if available) and their current behavior in conversation. This could indicate growth or areas for development.
""",
    )

    skill_judge_agent = create_skill_judge_agent()

    # TODO: Maybe this tool should return some judgement info for steering the convo?
    @agent.tool(retries=2)
    async def judge_conversation(
        ctx: RunContext[ChatDeps],
        recent_messages: int = -1,
    ) -> None:
        """Evaluate recent conversation for social skill demonstration.

        If recent_messages is -1, it will use all messages in the session.

        Args:
            ctx (RunContext[ChatDeps]): The agent context containing database access and session info.
            recent_messages (int): Number of recent messages to analyze (default: -1).

        Returns:
            None: This tool does not return a value, it only performs the evaluation and stores it in the database.
        """
        log.info(f"Agent called: judge_conversation(recent_messages={recent_messages})")

        db = ctx.deps.db
        session_id = ctx.deps.session_id

        # Get the current message history from the run context
        # We'll use the message history from the database for consistency
        message_history = await db.get_messages(session_id)

        if len(message_history) == 0:
            return

        # Perform skill evaluation.
        try:
            skill_evaluation = await evaluate_recent_conversation(
                skill_judge_agent=skill_judge_agent,
                message_history=message_history,
                recent_messages=recent_messages,
            )

            # Explicitly store the evaluation if we have a valid skill type and score
            if skill_evaluation.skill_type is not None:
                await db.add_skill_evaluation(
                    user_id=ctx.deps.user_id,
                    session_id=session_id,
                    judgment=skill_evaluation,
                )

                log.info(
                    f"Recorded skill evaluation for user {ctx.deps.user_id} in session {session_id}: {skill_evaluation.skill_type} = {skill_evaluation.score}"
                )
            else:
                log.info(f"Reason for no evaluation: {skill_evaluation.reason}")
        except Exception as e:
            log.error(f"Error in judge_conversation tool: {e}")

    # TODO: This was generated by Copilot, it isn't exactly what I want, but close enough to adapt later.
    # @agent.tool
    async def get_user_progress(ctx: RunContext[ChatDeps]) -> str:
        """Get a summary of the user's skill development progress."""
        db = ctx.deps.db
        session_id = ctx.deps.session_id

        try:
            progress = await get_user_skill_summary(ctx.deps)

            # Format the progress into a readable summary
            mastered_count = progress.mastered_skills
            total_count = progress.total_skills
            in_progress_count = progress.skills_in_progress

            summary = f"Progress Summary for this session:\n"
            summary += f"- Mastered skills: {mastered_count}/{total_count}\n"
            summary += f"- Skills in progress: {in_progress_count}\n"

            # Add details about top performing skills
            skill_details = progress.skill_details
            top_skills = sorted(
                [
                    (skill, details)
                    for skill, details in skill_details.items()
                    if details.total_evaluations > 0
                ],
                key=lambda x: x[1].weighted_score,
                reverse=True,
            )[:3]

            if top_skills:
                summary += "\nTop performing skills:\n"
                for skill, details in top_skills:
                    summary += f"- {skill}: {details.weighted_score:.2f} (mastered: {'yes' if details.is_mastered else 'no'})\n"

            return summary

        except Exception as e:
            log.error(f"Error in get_user_progress tool: {e}")
            return f"Unable to retrieve progress: {str(e)}"

    return agent
